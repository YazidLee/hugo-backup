[{"content":"HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型，除了线程不安全外，基本上等同于HashTable，但是它允许null作为键和值。HashMap的Key要求是不可变类型的，因为如果是可变类型的Key，那么在使用过程中很有可能对Key对象进行了修改，导致哈希值发生变化，最终无法定位到HashMap中的元素。\nJava8对HashMap进行了大修改，为了防止链表过大，影响插入和查找的效率(链表过大时，时间复杂度为O(n))，当链表元素的数量超过某个值时，自动将链表转换为红黑树(时间复杂度为O(log n)，注意这个地方有个坑，文章最后会介绍)\n基础结构 简单来说，HashMap就是一个数组，数组中的每个位置被称做bin或者bucket(中文翻译为桶)，每个桶中都存放着一些Node(结点)。当一个桶中的Node数量较少的时候，使用链表对Node进行存储；当一个桶中的Node数量超过某个阈值的时候，就会将链表转换为红黑树，这个操作叫做treeify，即\u0026quot;树化\u0026quot;，注意树化操作还需要满足另外一个条件，就是数组的长度要超过MIN_TREEIFY_CAPACITY = 64，否则它的操作就不是树化，而是resize。同样，在resize操作的时候，也会判断一个桶中的Node数量是否会少于某个阈值，如果满足条件，则会重新将红黑树转换回链表，这个操作称为untreeify。\n首先介绍几个重要的参数：\n capacity: 数组当前的最大长度，即为桶数量的最大值，最多存放多少个桶，这个数值在第一次添加元素的时候初始化为16。满足一定条件时，会扩容。这个长度必须是2的整数次方(16, 32, 64, 128 \u0026hellip;)，稍后在扩容章节会详细讲解其中的原因 loadFactor：负载因子，它搭配capacity使用，判断扩容条件，默认值为0.75 size：当前HashMap中的Node总数量 threshold：扩容阈值，它的值为loadFactor * capacity，重点：当size的值大于threshold值时，进行扩容  源码分析 table域-数组 这就是图中所示的那个数组，类型为Node：\n1  transient Node\u0026lt;K,V\u0026gt;[] table;   再看看Node的内容，很明显，就是一个简单的链表结构:\n1 2 3 4 5 6 7 8  static class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash; // 当前节点中Key的hash值，注意不是hashCode()的返回值，具体是什么会在扩容中介绍  final K key; // 键  V value; // 值  Node\u0026lt;K,V\u0026gt; next; // 下一个结点  ... }   当然还有其他类型的Node，如TreeNode，这里就不展示了\nput方法 当需要将元素存入HashMap时，我们使用V put(K, V)方法，它的作用是，若key已经存在，则用新的value替换原来的value；否则插入新的key和value，返回null(返回值为null也可以说明当前key在map中对应的值为null)。它的源码如下：\n1 2 3  public V put(K key, V value) { return putVal(hash(key), key, value, false, true); }   它调用了V putVal(int, K, V, boolean, boolean)方法，注意，这里还调用了一个hash方法，具体将在稍后的扩容中介绍，这里只需要明白它的作用是为了让Node分布更均匀。putVal方法的源码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; // 1. 当数组为空或者数组长度为0的时候，延迟初始化  if ((tab = table) == null || (n = tab.length) == 0) // 此时，resize将使用16作为初始容量，创建初始数组，n = size = 16，threshold = 16 * 0.75 = 12  n = (tab = resize()).length; // 2. 将hash映射到数组长度内，得到索引值，并判断tab在该处是否已经有Node  if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) // 若没有Node，则表明此次插入的Node为tab[i]这个桶上的第一个Node，直接将该Node赋值给tab[i]  tab[i] = newNode(hash, key, value, null); // 3. 数组不为空，且当前桶处已有其他Node  else { Node\u0026lt;K,V\u0026gt; e; K k; // 判断待插入的key与桶中的第一个Node的key是否相等，若相等，则稍后直接覆盖  if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) e = p; // 如果是红黑树结构，则进行红黑树的put操作  else if (p instanceof TreeNode) e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); // 如果是链表结构  else { // 遍历链表，直到找到元素，或者到达表尾  for (int binCount = 0; ; ++binCount) { // 到达表尾，没有找到与传入的key相等(hash和equals判断)的Node，则插入新结点  if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 若插入新结点后，当前桶中的链表结点数超过了8个，则转换为红黑树  if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st  treeifyBin(tab, hash); break; } // 在链表中找到了与传入key相等的Node，则直接退出遍历，并在稍后将值直接覆盖  if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } } // 4. e不为空表示HashMap中已有相同key的Node，直接将旧值替换成新值  if (e != null) { // existing mapping for key  V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); // 返回旧值  return oldValue; } } // 修改后，这个值递增，确保fast-fail机制  ++modCount; // 5. size自增，若此时size的值大于threshold，则进行扩容  if (++size \u0026gt; threshold) resize(); afterNodeInsertion(evict); return null; }   着重介绍其中几个关键流程(以下编号与代码注释中的编号对应)：\n 判断当前的数组是否为空(或长度为0)，若满足条件，则说明当前put的Node为当前HashMap对象中的第一个Node，就调用resize()方法创建一个数组。在之前的Java版本中，这个创建数组的操作都是在构造函数中完成的，这就导致了，即使不往HashMap里存东西，它也会占据内存空间。这里延迟到了添加第一个元素的时候，确保即使new了一个HashMap，且不往里面存东西，它也不会占据额外的内存空间。 这里先将一个hash值(32位int值)映射到数组的索引范围内，常见的做法是对素数取余，但这里利用位运算进行(n - 1) \u0026amp; hash(稍后再介绍这个操作)，这里将这个索引值记为index。随后判断这个索引位置上的桶是否已经有Node存在，若没有，则表示当前插入Node为该桶中的第一个Node，直接进行赋值操作tab[index] = newNode... 若index处的桶中已有Node，则需要进一步的判断：  若index处桶中的第一个Node的key就与待插入的key相等(hash和equals判断)，则不用进行后面的判断，准备进行直接的值覆盖 否则，判断当前Node的类型，若是TreeNode，表明它是红黑树，则进行红黑树的put操作 否则，表示当前Node为普通链表类型，对当前的链表进行遍历操作。若找到了key相等的Node，就break结束遍历，准备直接覆盖值；若找不到，则在链表尾插入相应的新Node，并判断当前链表的长度，若超过了阈值TREEIFY_THRESHOLD(8)，则将链表树化，转化成红黑树结构   在上述的插入过程中，若找到了key与待插入key相等的Node，则直接用新值对该Node的值进行覆盖，并直接返回旧值，结束整个插入过程 若没有找到key与待插入key相等的Node，则表明进行了插入操作，此时Node的总数量增加，即size自增。此时判断size的值是否大于扩容域值，若满足条件，则进行扩容，这个操作将在下面进行具体介绍。  值得注意的一点是，treeifyBin这个将列表转换为红黑树的方法，它在数组的总容量较小的情况下并不会真正将链表转换为红黑树，而是先进行resize扩容操作，具体代码如下：可以看到，在length小于MIN_TREEIFY_CAPACITY(64)的情况下，它直接调用了resize()方法，并没有直接将链表转换为树\n1 2 3 4 5 6 7 8  final void treeifyBin(Node\u0026lt;K,V\u0026gt;[] tab, int hash) { int n, index; Node\u0026lt;K,V\u0026gt; e; if (tab == null || (n = tab.length) \u0026lt; MIN_TREEIFY_CAPACITY) resize(); else if (...) { ... } }   扩容 扩容(resize) 就是重新计算容量、扩大数组容量以及将已有元素重新放置。若向HashMap对象里不停的添加元素，而HashMap对象内部的数组存储的元素达到一定数量时，就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的、更大的数组代替已有的容量小的数组。\n当然，“能够装入更多的元素”这个说法不太严谨，其实就算不扩容，理论上也能不停地加入元素，因为链表和红黑树都能无限扩展： HashMap的查询和插入效率很高，理论上能达到常数级别，但当每个桶中的Node都非常多，查询效率和插入效率就会大打折扣，每次查询或者插入需要比较的次数迅速增加，链表会退化为O(n)(JDK8之前)，而红黑树也需要O(logn)。因此，当Node过多，可以通过扩容的方式，将集中在同若干个桶中的Node分散到更多的桶中，用空间换取时间\n(length - 1) \u0026amp; hash 关于这个容量，首先要介绍一个重要的结论，它的值必须是一个2的正整数次幂。\n 当使用默认的构造方法(无参数)创建HashMap时，默认的初始容量为16(如上面所说，是在插入第一个元素时分配的)； 如果使用的是带参数的构造方法(带int值的)，那么就会先计算大于等于该值的、最小的一个2的正整数次幂(比如传入的int值为17，2^4 \u0026lt; 17 \u0026lt; 2^5，则其初始容量为2^5 = 32)，同样是延迟到第一次put时才创建数组。下面是相关的代码：  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  /** * The default initial capacity - MUST be a power of two. 默认初始容量-必须为2的幂 */ static final int DEFAULT_INITIAL_CAPACITY = 1 \u0026lt;\u0026lt; 4; // aka 16 ... public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; // tableSizeFor()方法将返回大于等于initialCapacity的2的整数幂，并将这个返回值赋值给threshold  // 在第一次调用resize()方法时，会将这个threshold作为初始容量创建数组，这个地方用法比较特殊  // 后续操作中threshold将一直作为判断是否扩容的标准，它的值为capacity*loadFactor  this.threshold = tableSizeFor(initialCapacity); }   所以为什么一定要是2的正整数次幂呢？这里又需要回顾上面putVal中，注释编号为2的代码处：\n1 2 3 4  ... n = tab.length ... ... if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) ...   上述代码中i = (n - 1) \u0026amp; hash很明显是在获取哈希值为hash的key所对应的数组索引(替代求余操作)，其中的n值为数组的length，所以这个索引实际上就是(length - 1) \u0026amp; hash。 将两个int类型的正整数进行按位与计算，结果不会超过两个数中的最小者，所以上面的操作结果不会超过length - 1，即结果范围为：[0, length - 1]，这就将hash映射到了数组的索引中。如下图所示，假定容量为64：\n随之而来的一个问题是，那为什么一定要是2的整数幂呢？任意一个容量(randomLength - 1) \u0026amp; hash进行按位与不也可以得到不超过容量的索引吗？现在假设不是2的整数次幂，比如62，如下图所示。绿色位置的值为0，此时进行按位与操作，不管hash中的红色部分值是0还是1，计算结果中相应位置上的值都是0。这意味着，计算后的索引结果中，不能取得*[0, 61]*这个区间内的所有值，有些值是不可能得到的，比如2, 3等等(因为结果的第二位是0，所以不可能是2和3)，也即HashMap中会有许多桶始终为空，造成了链表或者红黑树的高度增加，效率降低。因此，(length - 1)的二进制表示需要全部为1，也即length必须是2的整数幂。\nint hash(Object)方法 在put方法介绍时，还留了一个坑，那就是int hash(Object)方法，有了刚介绍的知识，就可以简单了解一下了：\n1 2 3 4 5 6 7 8 9 10  // put方法调用了putVal方法，putVal方法的第一个参数使用了hash方法，因此实际上传入putVal方法的hash值其实是经过处理的hashCode() public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } // hash方法 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); }   putVal方法的第一个参数使用了int hash(Object)方法，因此实际上传入putVal方法的hash值其实是经过再处理的哈希值，而不是直接使用我们代码中编写的hashCode()。这个方法是对程序员重写的hashCode()的一种优化，因为程序员编写的hashCode()目的是尽量让返回值在int(32位)范围内尽可能不同。然而，根据上一部分的分析，在数组容量length较小时，我们往往只使用到了低位的hash值，高位的hash值被忽略(此时length - 1的二进制高位均为0)，这很可能导致冲突较多。因此，int hash(Object)方法让高位与低位进行了一次异或运算，保证高位的值也能够体现在hash值中，能够有效减少冲突。\nresize()方法 啥都不说，先上源码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99  final Node\u0026lt;K,V\u0026gt;[] resize() { // 扩容前的数组  Node\u0026lt;K,V\u0026gt;[] oldTab = table; // 扩容前的容量，因为这个方法在第一次put的时候也会被调用，它需要考虑到为null的情况  int oldCap = (oldTab == null) ? 0 : oldTab.length; // 扩容前的扩容阈值threshold  int oldThr = threshold; // 扩容后的新容量和新阈值  int newCap, newThr = 0; // 如果旧的容量大于0，表示不是第一次put元素，HashMap中已有数据  if (oldCap \u0026gt; 0) { if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 1. 容量翻倍  else if ((newCap = oldCap \u0026lt;\u0026lt; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; // 如果旧容量比16小，则使用后面的方式计算新扩容阈值  oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) // 同时，新的容量阈值为旧容量阈值的两倍(因为新的容量翻倍了)  newThr = oldThr \u0026lt;\u0026lt; 1; // double threshold  } // 上文提到过，当使用带int参数的HaspMap构造方法时，就在这个地方将threshold作为初始化数组的大小  // 注意此处没有指定newThr，则时候稍后的方法进行计算  else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold  newCap = oldThr; else { // zero initial threshold signifies using defaults  // 同样在上文提到过，当使用无参数的HashMap构造方法时，就在这里指定默认的初始数组大小  newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 针对上文中没有计算newThr的情况，统一在这进行计算  if (newThr == 0) { // newThr的值为capacity * loadFactor  float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } // 将计算后的新阈值赋给threshold，表示扩容后的阈值  threshold = newThr; // 2. 创建大小为新容量的数组  @SuppressWarnings({\u0026#34;rawtypes\u0026#34;,\u0026#34;unchecked\u0026#34;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; // 将HashMap中的数组替换为新数组  table = newTab; // 将旧数组中的Node往新数组中搬  if (oldTab != null) { // 遍历旧数组中的各个桶  for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; // 如果桶中只有一个Node，则使用(length - 1) \u0026amp; hash的方法，直接将这个Node存到新数组的相应位置  if (e.next == null) newTab[e.hash \u0026amp; (newCap - 1)] = e; // 如果是红黑树，则进行红黑树相关操作  else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); // 3. 如果是链表，则遍历链表中的所有结点，确定其在新数组中的位置  else { // 4. preserve order  // 5. 将旧桶中的链表结点分成两类，分别组成两个链表，然后分别插入到新数组的特定桶中  Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; // 低位置的链表，记为lo  Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; // 高位置的链表，记为hi  Node\u0026lt;K,V\u0026gt; next; do { next = e.next; // 骚操作，表示e所指向的结点在新数组中的下标和旧数组中的下标相同，把这些Node放到lo链表中，原因还是因为数组容量为2的整数幂  if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 否则，表示e所指向的结点在新数组中的下标是旧数组中下标的2倍，把这些Node放到hi链表中  else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 分别将两个链表插入到低位和高位桶中  if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; }   相当复杂，还是和前面的putVal一样，列举几个关键点进行分析：\n  这里不讨论数组为空的情况，假设现在HashMap中已经有许多Node了，并且在插入完成某个新的Node后，触发了扩容(忘记了?快回头去看看putVal方法的第5点)，此时，把新的容量和新的扩容阈值都设为原来的2倍\n  随后，创建一个大小为新容量的数组，并将HashMap中的table重新赋值(这个也忘记了?请回到源码分析的开头部分)，这时候就已经完成了容量的翻倍，但旧数组中的Node都还没搬过来\n  由于红黑树较为复杂 (我还不会) ，这里只分析链表的情况\n  请注意这个地方有个很显眼的 preserve order ，这个是JDK开发人员的注释，为什么要特别加这么一条注释呢？其实是因为这个地方在Java8之前有个不算坑的坑，这里就稍微说明一下：由于Java8之前，采用的是链表的头插法，因此在扩容过程中，有可能导致链表结点之间的顺序改变，这在一般情况下并不是什么问题，但在多线程环境下，有概率出现循环链表，从而出现死循环的情况。有人就把这个问题反馈给了JDK开发人员，但是，HashMap的说明中明确指出了，HashMap是线程不安全的，所以当时也并没有对这个问题进行解决(这纯粹就是使用者的锅)。但是到了Java8，这个问题被重写HashMap的JDK开发人员顺手给解决了，他特地在这标注了一个 preserve order，表示已经解决了那个坑，有兴趣的话可以访问这个链接JAVA HASHMAP的死循环\n  这个位置就是要开始将旧数组中的链表搬到新数组的桶中了。与Java8之前的方法不同，在这段代码中，没有对每个Node重新进行hash值的计算(为了在新的数组中确定Node的索引值)，而是使用了(e.hash \u0026amp; oldCap) == 0这么一个熟悉的条件判断进行索引位置的确定。啧，怎么又是个按位与操作？之前我们使用过hash \u0026amp; (length - 1)确定索引值，而这里的(e.hash \u0026amp; oldCap) == 0又是个什么操作？仔细分析，不难发现，oldCap表示扩容前的容量，是一个2的整数幂的值，所以它的二进制表示为某个特定位上的值为1，其余位置全是0，用它和结点的hash值进行按位与，就是判断结点的hash值在那个对应的特定位置上是否为0。那这又有什么用呢？结合下面图片进行分析： ![](图中，上面两个二进制数表示的是扩容前的某个结的hash值和oldCap - 1；下面两个二进制数表示的是扩容后的同一个结点的hash值和newCap - 1。通过观察，由于扩容时容量加倍，使得newCap - 1比oldCap - 1多出了一位1(绿色的部分)，因此进行hash \u0026amp; (length - 1)时，hash中参与计算的位也多了一位(红色的部分)。这个位置hash的值不是0就是1，也就是说，hash \u0026amp; (newCap - 1)和hash \u0026amp; (oldCap - 1)的结果就差在这一位上(因为是计算同一个结点在新老数组中的索引位置，参与计算的hash值是相同的，而且容量减1的值在各个位上都是1)。所以我们就可以做出一个判断：\n 当红色部分的值为0时，新数组中的索引值newIndex和老数组中的索引值相同，即newIndex = oldIndex 当红色部分的值为1时，新数组中的索引值newIndex是老数组中的索引值的2倍，同时，由于新老数组的容量都刚好是2的整数幂，因此可以写成newIndex = oldIndex + oldCap的形式  有了这些分析，我们现在就可以知道，(e.hash \u0026amp; oldCap) == 0这个操作判断的就是新增位上(红色的部分)hash值的情况(0或1)。随后，把所有判断结果为0的结点连接成一个链表lo，把所有判断结果为1的结点连接成一个链表hi。最后，把lo放到新数组中索引与老数组相同的位置，而hi则被放入[oldIndex + oldCap]的位置。\n  HashMap和Comparable 最后，还有一个坑要填(即使实际开发中并不会遇到)。虽然对于红黑树的了解不算深入，但至少知道它是个动态平衡的二叉查找树，比大小是它核心的一个操作。随之而来的一个问题就是，HashMap不像TreeMap要求Key实现Comparable或者构造时提供Comparator，它对于Key没有这方面的限制，那它内部是通过什么来进行比较呢？下面先进行一个实验：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  class TestKey { private String id; private int number; public TestKey(String id, int number) { this.id = id; this.number = number; } /** * 故意写个冲突的hashCode * @return 返回固定的hashCode */ @Override public int hashCode() { return 5; } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; TestKey testKey = (TestKey) o; return number == testKey.number \u0026amp;\u0026amp; id.equals(testKey.id); } } public class HashMapDemo { private static final int COUNT = 500_000; public static void main(String[] args) { HashMap\u0026lt;TestKey, Integer\u0026gt; testMap = new HashMap\u0026lt;\u0026gt;(); Integer testValue = -1; TestKey key = null; long startTime = System.currentTimeMillis(); for (int i = 0; i \u0026lt; COUNT; i++) { key = new TestKey(\u0026#34;test\u0026#34;, i); testMap.put(key, i); } long endTime = System.currentTimeMillis(); System.out.println(\u0026#34;插入耗时:\u0026#34; + (endTime - startTime)); } }   执行结果：\n1  插入耗时:4875921   上述代码中的TestKey作为HashMap的键，且刻意让它的hashCode()方法返回固定值，在put的时候不断产生冲突。main方法中进行了50万次put，花了1个多小时才完成。\n现在修改TestKey类，让它实现Comparable接口，其他代码不变：\n1 2 3 4 5 6 7  class TestKey implements Comparable\u0026lt;TestKey\u0026gt; { ... @Override public int compareTo(TestKey o) { return number - o.number; } }   再次执行，看看结果：\n1  插入耗时:155   可以看到，其他操作都不变，仅仅多实现了一个Comparable，put的执行时间就显著缩短了。由于这么多数据都在同一个桶中，它们的结构肯定是红黑树，因此，直接分析putTreeVal方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  final TreeNode\u0026lt;K,V\u0026gt; putTreeVal(HashMap\u0026lt;K,V\u0026gt; map, Node\u0026lt;K,V\u0026gt;[] tab, int h, K k, V v) { Class\u0026lt;?\u0026gt; kc = null; boolean searched = false; TreeNode\u0026lt;K,V\u0026gt; root = (parent != null) ? root() : this; for (TreeNode\u0026lt;K,V\u0026gt; p = root;;) { int dir, ph; K pk; if ((ph = p.hash) \u0026gt; h) dir = -1; else if (ph \u0026lt; h) dir = 1; else if ((pk = p.key) == k || (k != null \u0026amp;\u0026amp; k.equals(pk))) return p; else if ((kc == null \u0026amp;\u0026amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) { if (!searched) { TreeNode\u0026lt;K,V\u0026gt; q, ch; searched = true; if (((ch = p.left) != null \u0026amp;\u0026amp; (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null \u0026amp;\u0026amp; (q = ch.find(h, k, kc)) != null)) return q; } dir = tieBreakOrder(k, pk); } ... } } }   能够发现，在红黑树中插入数据时，先判断hash值，当hash值相等，且equals判断也相等，则会判断是否为Comparable；若Key实现了Comparable，则直接使用compareTo方法进行大小判断；若连Comparable都没实现(或者compareTo方法判断为相等时)，则会调用tieBreakOrder方法，这个方法中使用System.identityHashCode进一步分析。对于50w个数据来说，System.identityHashCode调用相当耗时，从上面的例子中可以看到，花了1个多小时。\n因此，虽然Java8对HashMap进行了优化，使过长的链表优化成红黑树，但如果Key的hashCode算法不佳，且Key没有实现Comparable接口，那么仍然有可能引发很糟糕的后果。在HashMap源码中，编写者有这么一段话：\n If neither of these apply, we may waste about a factor of two in time and space compared to taking no precautions. But the only known cases stem from poor user programming practices that are already so slow that this makes little difference.\n 大概意思是：如果两者都不满足(指良好的hashCode方法和实现Comparable接口)，那么HashMap的新实现(红黑树)会浪费两倍的空间和时间。但是这种极端的情况是由开发者不良的编程实现引起的，其实用什么实现(链表或者红黑树)已经没区别了。\n","permalink":"https://www.liyangjie.cn:4443/posts/hashmap%E5%9F%BA%E7%A1%80/","summary":"HashMap是Java程序员使用频率最高的用于映射(键值对)处理的数据类型，除了线程不安全外，基本上等同于HashTable，但是它允许null作为键和值。HashMap的Key要求是不可变类型的，因为如果是可变类型的Key，那么在使用过程中很有可能对Key对象进行了修改，导致哈希值发生变化，最终无法定位到HashMap中的元素。\nJava8对HashMap进行了大修改，为了防止链表过大，影响插入和查找的效率(链表过大时，时间复杂度为O(n))，当链表元素的数量超过某个值时，自动将链表转换为红黑树(时间复杂度为O(log n)，注意这个地方有个坑，文章最后会介绍)\n基础结构 简单来说，HashMap就是一个数组，数组中的每个位置被称做bin或者bucket(中文翻译为桶)，每个桶中都存放着一些Node(结点)。当一个桶中的Node数量较少的时候，使用链表对Node进行存储；当一个桶中的Node数量超过某个阈值的时候，就会将链表转换为红黑树，这个操作叫做treeify，即\u0026quot;树化\u0026quot;，注意树化操作还需要满足另外一个条件，就是数组的长度要超过MIN_TREEIFY_CAPACITY = 64，否则它的操作就不是树化，而是resize。同样，在resize操作的时候，也会判断一个桶中的Node数量是否会少于某个阈值，如果满足条件，则会重新将红黑树转换回链表，这个操作称为untreeify。\n首先介绍几个重要的参数：\n capacity: 数组当前的最大长度，即为桶数量的最大值，最多存放多少个桶，这个数值在第一次添加元素的时候初始化为16。满足一定条件时，会扩容。这个长度必须是2的整数次方(16, 32, 64, 128 \u0026hellip;)，稍后在扩容章节会详细讲解其中的原因 loadFactor：负载因子，它搭配capacity使用，判断扩容条件，默认值为0.75 size：当前HashMap中的Node总数量 threshold：扩容阈值，它的值为loadFactor * capacity，重点：当size的值大于threshold值时，进行扩容  源码分析 table域-数组 这就是图中所示的那个数组，类型为Node：\n1  transient Node\u0026lt;K,V\u0026gt;[] table;   再看看Node的内容，很明显，就是一个简单的链表结构:\n1 2 3 4 5 6 7 8  static class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash; // 当前节点中Key的hash值，注意不是hashCode()的返回值，具体是什么会在扩容中介绍  final K key; // 键  V value; // 值  Node\u0026lt;K,V\u0026gt; next; // 下一个结点  ... }   当然还有其他类型的Node，如TreeNode，这里就不展示了","title":"HashMap基础"},{"content":"","permalink":"https://www.liyangjie.cn:4443/posts/githooktest4/","summary":"","title":"GitHookTest4"},{"content":"","permalink":"https://www.liyangjie.cn:4443/posts/githooktest3/","summary":"","title":"GitHookTest3"},{"content":"","permalink":"https://www.liyangjie.cn:4443/posts/githooktest2/","summary":"","title":"GitHookTest2"},{"content":"Git Hook Demo ","permalink":"https://www.liyangjie.cn:4443/posts/githooktest/","summary":"Git Hook Demo ","title":"GitHookTest"},{"content":"HelloHugo 这是我的第一个Hugo测试\n","permalink":"https://www.liyangjie.cn:4443/posts/hellohugo/","summary":"HelloHugo 这是我的第一个Hugo测试","title":"HelloHugo"}]